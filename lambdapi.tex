
LambdaPi is an implementation of the lambda-pi calculus modulo ($\lp$), so we
will first quickly describe this system, then describe the Martin-Löf Type
Theory (MLTT) we implemented, along with some property, and finally we describe
how it is embedded into LambdaPi.

\subsection{Lambda-Pi Calculus Modulo}

The lambda-pi calculus modulo is an extension on the lambda-pi calculus, so
first let's describe it. The terms are the usual terms of the $\lambda$-calculus with
dependent types~:

\[\begin{array}{rcl}
    t, u, v, T, U\dots & := & \lambda(x : T), t \\
    & | & x \\
    & | & \Pi(x : T), U \\
    & | & u\ v \\
    & | & \text{TYPE} \\
 \end{array}\]

The typing rule for abstractions is the following~:

\begin{center}\begin{prooftree}
    \hypo{\Gamma\vdash U : \text{TYPE}}
    \hypo{\Gamma, x: U\vdash t : T}
    \infer2{\Gamma\vdash \lambda(x :  U), t : \Pi(x : U), T}
\end{prooftree}\end{center}

The other typing rules are without surprise. Let's just focus on one perhaps
surprising aspect of the abstraction rule. The type of the argument must be of
type $\text{TYPE}$. But since $\Gamma\vdash\text{TYPE} : \text{TYPE}$ does not hold, as it
would make the system inconsistent, it is impossible to quantify over all types.
This is a limitation that other type systems try to avoid using hierarchies of
universes, but it make the system more complicated. While it seems a bit too
constraining not to have polymorphism, we will see in section \ref{codes} how we
can still simulate polymorphism using the modulo part of the type theory.

Now let's focus on another rule, that is standard in type theory, the conversion
rule~:

\begin{center}\begin{prooftree}
    \hypo{\Gamma\vdash t : T}
    \hypo{\Gamma\vdash T \equiv U}
    \infer2{\Gamma\vdash t : U}
\end{prooftree}\end{center}

Here $\Gamma\vdash T \equiv U$ means that $T$ and $U$ are convertible, in the case of LambdaPi
$\alpha\beta$-convertible. This is pretty standard stuff, but that's where the
\emph{modulo} part of lambda-pi calculus modulo comes in. Indeed, this systems
allows the context to extend the congruence used in the conversion rule, ie $\Gamma$
can contain new equalities that are used when applying the congruence rule. One
problem is that just doing it naively can make the type checking undecidable,
and even make the logic inconsistant. However, there is one specific case that
is both decidable and proved consistant~: when $\equiv$ is the symmetric, reflexive
and transitive closure of a strongly-normalizing confluent system $\hookrightarrow$.

This is what the current implementation of LambdaPi does~: it provides a syntax
to declare new symbols and reduction rules about those symbols, and then provide
a type-checking algorithm that works modulo the congruence generated by the
reduction rules. However, it is not able to check the confluence nor
normalisation of the system, so the proof obligation of those lies to the user.

\subsubsection{Very simple sum type}

As an example, assume we have $T$ and $U$ two types in our context, and we want
to create the type $T + U$ in LambdaPi. We would need so create a type $S$ that
would be isomorphic to the sum, then declare symbols for the constructors and
the elimination. Of course, since we cannot quantify over types, the elimination
needs to be into a specific type, let's say $C$.  Finally, we would need to add
computation rules so that the elimination of the constructors behave as
expected. This gives us the following code~:

\begin{lstlisting}
  symbol S : TYPE;
  symbol i1 : T → S;
  symbol i2 : U → S;
  symbol elim : (T → C) → (U → C) → S → C;
  rule elim $c1 _ (i1 $t) ↪ $c1 $t
  with elim _ $c2 (i2 $u) ↪ $c2 $u;
\end{lstlisting}

\subsubsection{Type codes}\label{codes}

The main limitation we have had is the absence of polymorphism, since we cannot
quantify over types. But we can quantify over types, so we can for example
create a type of codes of types, along with an interpretation function, and then
add codes for all the types that we want to represent. The basic framework would
be something like~:

\begin{lstlisting}
  symbol Set : TYPE;
  symbol El : Set → TYPE;
\end{lstlisting}

Now we can add a product type in $Set$~:

\begin{lstlisting}
  symbol P (A : Set) (B : A → Set) : Set;
  rule El (P $A $B) ↪ Π(x : El $A), $B x;
\end{lstlisting}

This way, $P$ is a code in $Set$ for the LambdaPi product type. But where this
approach really shines is that we can add codes for types that have no
equivalent in dedukti, and still use them naturally. For example, if we want to
add a polymorphic binary sum in $Set$, we can do it~:

\begin{lstlisting}
  symbol S : Set → Set → Set;
  symbol i1 (U V : Set) : El U → El (S U V);
  symbol i2 (U V : Set) : El V → El (S U V);
  symbol elim (U V C : Set) : (El U → El C)
                            → (El V → El C)
                            → El (S U V) → El C;
  rule elim _ _ _ $c1 _ (i1 _ _ $x) ↪ $c1 $x
  with elim _ _ _ _ $c2 (i2 _ _ $x) ↪ $c2 $x;
\end{lstlisting}

And now we have a sum type in $Set$ that is polymorphic over $Set$, but thanks
to the reduction rules it acts as a primitive sum type. And thus despite the
fact that there is no primitive sum type in LambdaPi. So this is the trick used
to have polymorphism in LambdaPi, and more generally to embed a logic that has
more primitive types than LambdaPi.

\subsection{Martin-Löf Type Theory}

\subsubsection{Definition}

\subsubsection{Two levels of equality}

\subsubsection{Uniqueness of identity proofs}

\subsubsection{Functional extensionality}

\subsection{Embedding MLTT in $\lp$}
